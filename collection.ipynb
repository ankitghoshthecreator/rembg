{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a9a543f-f0e7-489c-8896-ec7fbe57b36c",
   "metadata": {},
   "source": [
    "## only pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caa389ff-3035-41c0-a207-050ae2d707fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your name:  ank\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving frames to: E:\\removeBackground\\ank\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Step 1: Ask for a name\n",
    "name = input(\"Enter your name: \")\n",
    "\n",
    "# Step 2: Create folder\n",
    "folder_path = os.path.join(os.getcwd(), name)\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "print(f\"Saving frames to: {folder_path}\")\n",
    "\n",
    "# Step 3: Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Step 4: Start Pose with segmentation\n",
    "with mp_pose.Pose(static_image_mode=False,\n",
    "                  model_complexity=1,\n",
    "                  enable_segmentation=True,\n",
    "                  min_detection_confidence=0.5,\n",
    "                  min_tracking_confidence=0.5) as pose:\n",
    "\n",
    "    frame_count = 0  # To save frames with unique names\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Pose estimation\n",
    "        results = pose.process(image)\n",
    "\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Create white background\n",
    "        bg_color = (255, 255, 255)\n",
    "        bg_image = np.zeros(image.shape, dtype=np.uint8)\n",
    "        bg_image[:] = bg_color\n",
    "\n",
    "        if results.segmentation_mask is not None:\n",
    "            condition = results.segmentation_mask > 0.02\n",
    "            output_image = np.where(condition[..., None], image, bg_image)\n",
    "        else:\n",
    "            output_image = image\n",
    "\n",
    "        # Draw landmarks if present\n",
    "        if results.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                output_image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "        # Step 5: Save each frame\n",
    "        filename = os.path.join(folder_path, f\"frame_{frame_count:04d}.jpg\")\n",
    "        cv2.imwrite(filename, output_image)\n",
    "        frame_count += 1\n",
    "\n",
    "        # Show preview\n",
    "        cv2.imshow('Pose with Background Removed', output_image)\n",
    "\n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf1cb65-f425-4bbc-9913-e4f0ccbdf2a1",
   "metadata": {},
   "source": [
    "## hand + pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80008cd2-e138-41c4-b6b7-72d88bfef055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your name:  ankit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving frames to: E:\\removeBackground\\ankit\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'enable_segmentation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 27\u001b[0m\n\u001b[0;32m     19\u001b[0m cap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Use both Pose and Hands\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mp_pose\u001b[38;5;241m.\u001b[39mPose(static_image_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     23\u001b[0m                   model_complexity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     24\u001b[0m                   enable_segmentation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     25\u001b[0m                   min_detection_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[0;32m     26\u001b[0m                   min_tracking_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pose, \\\n\u001b[1;32m---> 27\u001b[0m      \u001b[43mmp_hands\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHands\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatic_image_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmax_num_hands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m                    \u001b[49m\u001b[43menable_segmentation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmin_detection_confidence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmin_tracking_confidence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m hands:\n\u001b[0;32m     33\u001b[0m     frame_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened():\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'enable_segmentation'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Step 1: Ask for a name\n",
    "name = input(\"Enter your name: \")\n",
    "\n",
    "# Step 2: Create folder\n",
    "folder_path = os.path.join(os.getcwd(), name)\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "print(f\"Saving frames to: {folder_path}\")\n",
    "\n",
    "# Initialize MediaPipe modules\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Use both Pose and Hands\n",
    "with mp_pose.Pose(static_image_mode=False,\n",
    "                  model_complexity=1,\n",
    "                  enable_segmentation=True,\n",
    "                  min_detection_confidence=0.5,\n",
    "                  min_tracking_confidence=0.5) as pose, \\\n",
    "     mp_hands.Hands(static_image_mode=False,\n",
    "                    max_num_hands=2,\n",
    "                    min_detection_confidence=0.5,\n",
    "                    min_tracking_confidence=0.5) as hands:\n",
    "\n",
    "    frame_count = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert to RGB\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image_rgb.flags.writeable = False\n",
    "\n",
    "        # Step 1: Pose estimation\n",
    "        pose_results = pose.process(image_rgb)\n",
    "\n",
    "        # Step 2: Hand detection\n",
    "        hand_results = hands.process(image_rgb)\n",
    "\n",
    "        image_rgb.flags.writeable = True\n",
    "        image_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Create white background\n",
    "        bg_image = np.ones(image_bgr.shape, dtype=np.uint8) * 255\n",
    "\n",
    "        # Use pose segmentation mask for background removal\n",
    "        if pose_results.segmentation_mask is not None:\n",
    "            condition = pose_results.segmentation_mask > 0.02\n",
    "            output_image = np.where(condition[..., None], image_bgr, bg_image)\n",
    "        else:\n",
    "            output_image = image_bgr\n",
    "\n",
    "        # Draw pose landmarks\n",
    "        if pose_results.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                output_image,\n",
    "                pose_results.pose_landmarks,\n",
    "                mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "        # Draw hand landmarks\n",
    "        if hand_results.multi_hand_landmarks:\n",
    "            for hand_landmarks in hand_results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    output_image,\n",
    "                    hand_landmarks,\n",
    "                    mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        # Save frame to folder\n",
    "        filename = os.path.join(folder_path, f\"frame_{frame_count:04d}.jpg\")\n",
    "        cv2.imwrite(filename, output_image)\n",
    "        frame_count += 1\n",
    "\n",
    "        # Show preview\n",
    "        cv2.imshow('Pose + Hands with Background Removed', output_image)\n",
    "\n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85802a0e-aa56-47ef-a94b-b9ac1dd0220d",
   "metadata": {},
   "source": [
    "## no bg remove and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34ddd07b-b1d7-4178-b3a8-587ef8bc7db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your name:  hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving frames to: E:\\removeBackground\\hello\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Step 1: Ask for a name\n",
    "name = input(\"Enter your name: \")\n",
    "\n",
    "# Step 2: Create folder\n",
    "folder_path = os.path.join(os.getcwd(), name)\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "print(f\"Saving frames to: {folder_path}\")\n",
    "\n",
    "# Initialize MediaPipe modules\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Use both Pose and Hands (NO segmentation needed)\n",
    "with mp_pose.Pose(static_image_mode=False,\n",
    "                  model_complexity=1,\n",
    "                  enable_segmentation=False,  # ✅ Segmentation turned off\n",
    "                  min_detection_confidence=0.5,\n",
    "                  min_tracking_confidence=0.5) as pose, \\\n",
    "     mp_hands.Hands(static_image_mode=False,\n",
    "                    max_num_hands=2,\n",
    "                    min_detection_confidence=0.5,\n",
    "                    min_tracking_confidence=0.5) as hands:\n",
    "\n",
    "    frame_count = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert to RGB\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image_rgb.flags.writeable = False\n",
    "\n",
    "        # Step 1: Pose estimation\n",
    "        pose_results = pose.process(image_rgb)\n",
    "\n",
    "        # Step 2: Hand detection\n",
    "        hand_results = hands.process(image_rgb)\n",
    "\n",
    "        image_rgb.flags.writeable = True\n",
    "        image_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # We skip background removal and use original image directly\n",
    "        output_image = image_bgr.copy()\n",
    "\n",
    "        # Draw pose landmarks\n",
    "        if pose_results.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                output_image,\n",
    "                pose_results.pose_landmarks,\n",
    "                mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "        # Draw hand landmarks\n",
    "        if hand_results.multi_hand_landmarks:\n",
    "            for hand_landmarks in hand_results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    output_image,\n",
    "                    hand_landmarks,\n",
    "                    mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        # Save frame to folder\n",
    "        filename = os.path.join(folder_path, f\"frame_{frame_count:04d}.jpg\")\n",
    "        cv2.imwrite(filename, output_image)\n",
    "        frame_count += 1\n",
    "\n",
    "        # Show preview\n",
    "        cv2.imshow('Pose + Hands (No Background Removed)', output_image)\n",
    "\n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "806afd9a-cac4-4572-b0f9-b30d6e461297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the full path of the folder with images:  ag\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ag\\frame_0000.jpg\n",
      "Processing: ag\\frame_0001.jpg\n",
      "Processing: ag\\frame_0002.jpg\n",
      "Processing: ag\\frame_0003.jpg\n",
      "Processing: ag\\frame_0004.jpg\n",
      "Processing: ag\\frame_0005.jpg\n",
      "Processing: ag\\frame_0006.jpg\n",
      "Processing: ag\\frame_0007.jpg\n",
      "Processing: ag\\frame_0008.jpg\n",
      "Processing: ag\\frame_0009.jpg\n",
      "Processing: ag\\frame_0010.jpg\n",
      "Processing: ag\\frame_0011.jpg\n",
      "Processing: ag\\frame_0012.jpg\n",
      "Processing: ag\\frame_0013.jpg\n",
      "Processing: ag\\frame_0014.jpg\n",
      "Processing: ag\\frame_0015.jpg\n",
      "Processing: ag\\frame_0016.jpg\n",
      "Processing: ag\\frame_0017.jpg\n",
      "Processing: ag\\frame_0018.jpg\n",
      "Processing: ag\\frame_0019.jpg\n",
      "Processing: ag\\frame_0020.jpg\n",
      "Processing: ag\\frame_0021.jpg\n",
      "Processing: ag\\frame_0022.jpg\n",
      "Processing: ag\\frame_0023.jpg\n",
      "Processing: ag\\frame_0024.jpg\n",
      "Processing: ag\\frame_0025.jpg\n",
      "Processing: ag\\frame_0026.jpg\n",
      "Processing: ag\\frame_0027.jpg\n",
      "Processing: ag\\frame_0028.jpg\n",
      "Processing: ag\\frame_0029.jpg\n",
      "Processing: ag\\frame_0030.jpg\n",
      "Processing: ag\\frame_0031.jpg\n",
      "Processing: ag\\frame_0032.jpg\n",
      "Processing: ag\\frame_0033.jpg\n",
      "Processing: ag\\frame_0034.jpg\n",
      "Processing: ag\\frame_0035.jpg\n",
      "Processing: ag\\frame_0036.jpg\n",
      "Processing: ag\\frame_0037.jpg\n",
      "Processing: ag\\frame_0038.jpg\n",
      "Processing: ag\\frame_0039.jpg\n",
      "Processing: ag\\frame_0040.jpg\n",
      "Processing: ag\\frame_0041.jpg\n",
      "Processing: ag\\frame_0042.jpg\n",
      "Processing: ag\\frame_0043.jpg\n",
      "Processing: ag\\frame_0044.jpg\n",
      "Processing: ag\\frame_0045.jpg\n",
      "Processing: ag\\frame_0046.jpg\n",
      "Processing: ag\\frame_0047.jpg\n",
      "Processing: ag\\frame_0048.jpg\n",
      "Processing: ag\\frame_0049.jpg\n",
      "Processing: ag\\frame_0050.jpg\n",
      "All images processed.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Ask for the folder path\n",
    "folder_path = input(\"Enter the full path of the folder with images: \").strip()\n",
    "\n",
    "# Check if folder exists\n",
    "if not os.path.isdir(folder_path):\n",
    "    print(\"Folder does not exist. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize MediaPipe Pose with segmentation\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "with mp_pose.Pose(static_image_mode=True,  # Use static mode for images\n",
    "                  model_complexity=1,\n",
    "                  enable_segmentation=True,\n",
    "                  min_detection_confidence=0.5) as pose:\n",
    "\n",
    "    # Process each image file\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            print(f\"Processing: {file_path}\")\n",
    "\n",
    "            # Read image\n",
    "            image = cv2.imread(file_path)\n",
    "            if image is None:\n",
    "                print(f\"Failed to read {file_path}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Convert to RGB\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            results = pose.process(image_rgb)\n",
    "\n",
    "            # Create white background\n",
    "            bg_image = np.ones(image.shape, dtype=np.uint8) * 255\n",
    "\n",
    "            # If segmentation mask is available\n",
    "            if results.segmentation_mask is not None:\n",
    "                condition = results.segmentation_mask > 0.07\n",
    "                output_image = np.where(condition[..., None], image, bg_image)\n",
    "            else:\n",
    "                output_image = image\n",
    "\n",
    "            # Overwrite the original image\n",
    "            cv2.imwrite(file_path, output_image)\n",
    "\n",
    "    print(\"All images processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "417242b1-5917-4a7c-907a-59ede555a27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in c:\\users\\ankit\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (10.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install pillo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7684cc99-f62d-4070-9eba-980b8f29d90c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
